---
title: "QTA Practicals Week 4 Lab Session"
date: "24 October 2025"
output: html_document
---

### Exercise 1 Dictionaries

```{r, echo = FALSE}
suppressPackageStartupMessages(library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE))
```

#### Getting used to dictionaries

Dictionaries are named lists, consisting of a "key" and a set of entries defining the equivalence class for the given key. To create a simple dictionary of parts of speech, for instance we could define a dictionary consisting of articles and conjunctions, using the `dictionary()` constructor

```{r}
posDict <- dictionary(list(articles = c("the", "a", "an"),
                           conjunctions = c("and", "but", "or", "nor", "for", "yet", "so")))
```

You can examine this dictionary by invoking its `print` method, simply by typing the name of the object and pressing Enter. What is the structure of this object?  (Hint: use `str()`.)

To let this define a set of features, we can use this dictionary when we create a `dfm`, for instance with the embedded inaugural corpus:

```{r}
toks <- tokens(data_corpus_inaugural)
inauguraldfm <- dfm(toks)

posDfm <- dfm_lookup(inauguraldfm, dictionary = posDict)
head(posDfm)
```

Weight the `posDfm` by relative term frequency, and plot the values of articles and conjunctions (actually, here just the coordinating conjunctions) across the speeches. (**Hint:** you can use `docvars(data_corpus_inaugural, "Year"))` for the *x*-axis.)

Is the distribution of normalized articles and conjunctions relatively constant across years, as you would expect?

```{r}
inauguralWeight <- dfm_weight(inauguraldfm, scheme = "prop")
posDfmWeight <- dfm_lookup(inauguralWeight, dictionary = posDict)
head(posDfmWeight)
```

```{r}
# Base R plot
plot(x = docvars(data_corpus_inaugural, "Year"), 
     y = posDfmWeight[, "articles"],
     type = "p", pch = 16, col = "orange",
     ylim = range(posDfmWeight),
     xlab = "Year", ylab = "Relative term frequency")
points(x = docvars(data_corpus_inaugural, "Year"), 
     y = posDfmWeight[, "conjunctions"],
     pch = 3, col = "blue", new = FALSE)
```

```{r}
# Plot with easier to see trends
library("ggplot2")
library("tidyr")
library("reshape2")
pdw <- convert(posDfmWeight, to = "data.frame")
pdw$year <- as.numeric(substr(pdw$doc_id, 1, 4))
pdw <- melt(pdw, id.vars = c("year", "doc_id"))
ggplot(pdw, aes(x = year, y = value, colour = variable)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(x = "Year", y = "Relative term frequency")
```

Either graphic shows that the usage of conjunctions is fairly stable over time although there seems to be an increasing trend since about the 1960s. The usage of articles was fairly stable until the end of the 19th century. Since then there is a steady decrease which seems to have level out over the last couple of decades.
        

#### Laver and Garry (2000) ideology dictionary
    
Here, we will apply the dictionary of Laver, Michael, and John Garry. 2000. "Estimating Policy Positions From Political Texts." *American Journal of Political Science* 44(3): 619–34.  Using the pre-built Laver and Garry (2000) dictionary file, which is distributed by Provalis Research for use with its [Wordstat software package from Provalis](http://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/laver-garry-dictionary-of-policy-position/), we will apply this to the same manifestos from the UK manifesto set. To do this, you will need to:

*   download and save the Wordstat-formatted dictionary file [LaverGarry.cat](http://www.kenbenoit.net/courses/essex2014qta/LaverGarry.cat);
    
*   load this into a dictionary list using `dictionary(file = "LaverGarry.cat", format = "wordstat")`;

*   build a dfm for the corpus subset for the Labour, Liberal Democrat, and Conservative Party manifestos from 1992 and 1997; and
    
*   try to replicate their measures from the "Computer" column of Table 2, for Economic Policy.  (Not as easy as you thought---any ideas as to why?)
    

```{r}
# You can also do this by hand!
# (This downloads and unzips the file only if it does not exist yet)
# if (!file.exists("LaverGarry.cat")) {
#   download.file("https://provalisresearch.com/Download/LaverGarry.zip", "LaverGarry.zip")
#   unzip("LaverGarry.zip")
# }
#install.packages("devtools")
#devtools::install_github("quanteda/quanteda.corpora")

library(quanteda.corpora)
data_corpus_ukmanifestos

LGDict <- quanteda::dictionary(file = "LaverGarry.cat", format = "wordstat")
LGCorpus <- corpus_subset(data_corpus_ukmanifestos, 
                             (Year %in% c(1992, 1997) & 
                                  Party %in% c("Lab", "LD", "Con")))
toks <- tokens(LGCorpus)
LGDfm <- dfm(toks)
```
 
```{r}
LGDfmDict <- dfm_lookup(LGDfm, dictionary = LGDict)
LGResult <- cbind(
  document = docnames(LGDfmDict), 
  as.data.frame(as.matrix(LGDfmDict), row.names = FALSE)
)

# Create scale
LGResult$econ <- (LGResult$`ECONOMY.-STATE-` - LGResult$`ECONOMY.+STATE+`) /
  (LGResult$`ECONOMY.+STATE+` + LGResult$`ECONOMY.-STATE-`)
LGResult$econStd <- scale(LGResult$econ)

LGResult[, c("document", "econ", "econStd")]
```

The differences to the results in the paper are on the one hand due to different standardization, as Laver & Garry use their full sample of values, i.e. including the Irish manifestos from two elections (p. 629). On the other hand, slightly different tokenization rules between `quanteda` and Wordstat might drive smaller differences between the two results.


#### Sentiment detection using dictionary methods

We will then look at a corpus of tweets posted by the four leading candidates in the 2016 US Presidential primaries. These are contained in a CSV file called `candidate-tweets.csv` available on CAVAS. One of the most common applications of dictionary methods is sentiment analysis: using a dictionary of positive and negative words, we compute a sentiment score for each individual document.

Let's apply this technique to tweets by the four leading candidates in the 2016 US Presidential primaries. Which candidate was using positive rhetoric most frequently? Which candidate was most negative in their public messages on Twitter?

```{r}
library("tidyverse")
library("quanteda")
tweets <- read_csv('candidate-tweets.csv')
```

We will use the positive and negative categories in the augmented General Inquirer dictionary to measure the extent to which these candidates adopted a positive or negative tone during the election campaign.

Note that first you will need to install the `quanteda.sentiment` package from GitHub

```{r, eval=FALSE}
devtools::install_github("quanteda/quanteda.sentiment")
devtools::install_github("kbenoit/quanteda.dictionaries") 
```

First, we load the dictionary object. Note that we can apply the dictionary object directly (as we will see later), but for now let's learn how to do this if we had a list of positive and negative words on a different file.

```{r}
library("quanteda.sentiment")
library("quanteda.dictionaries")
data(data_dictionary_geninqposneg)

pos.words <- data_dictionary_geninqposneg[['positive']]
neg.words <- data_dictionary_geninqposneg[['negative']]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
```

##### Calculating sentiment manually

We will eventually use `quanteda` to apply dictionaries quickly. But before we do that, let's do it manually. This is meant to help you understand the intuition behind applying dictionaries, as well as understand how to do it "by hand".

For all the analysis in the remainder of this file, we won't do many of the "standard" preprocessing steps to our corpus before using the dictionary. Depending on your task, you may or may not want to do this. For example, you may want to stem words, or drop stop words before applying dictionaries. Below, we'll see how weighting will affect our results. But as always, you will need to make these choices thoughtfully when you do your own QTA analyses.

One thing we _do_ want to do to our corpus is to make it entirely lowercase, since the dictionary stores its values in lowercase. Let's create a new variable.

```{r}
tweets$ltext <- str_to_lower(tweets$text) 
```

We will create some R code that iterates over every tweet and counts the number of positive words and the number of negative words in that tweet. We'll store the counts as new variables in the dataframe. 

Since there are a lot of tweets in this corpus, this code will take a long time to run, so we'll just take a random sample of 100 tweets from each candidate as proof of concept. 

```{r}
set.seed(2025) # set the seed if you get the same results every time you run this
small.tweets <- tweets %>% 
  group_by(screen_name) %>%
  slice_sample(n=100)
```

Now, let's "manually" count positive and negative words. We use the `sapply` function here, which does the same thing as a loop but much faster and with more compact code. 

```{r}
# First create a function that takes in text and a token list and counts how many times tokens from the token list appear in the text
count.toks <- function(text, tok_list){
  t.count <- sapply(tok_list, function(x) str_count(text, x))
  return(sum(t.count))
}

small.tweets$pos.words <- unname(sapply(small.tweets$ltext, function(x) count.toks(x, pos.words)))
small.tweets$neg.words <- unname(sapply(small.tweets$ltext, function(x) count.toks(x, neg.words)))
```

Let's also count the total number of words in each tweet. For this, we'll just break up by white space.

```{r}
small.tweets$total.words <- sapply(str_split(small.tweets$ltext, "\\s+"), length)
```

Finally, let's calculate the proportion of each tweet that is positive and the proportion that is negative, and show averages by candidate.

```{r}
small.tweets$prop.pos <- small.tweets$pos.words/small.tweets$total.words
small.tweets$prop.neg <- small.tweets$neg.words/small.tweets$total.words
small.tweets[1:10,c('text', 'prop.pos', 'prop.neg', 'text')]
```

Finally, let's calculate the percentage of positive words and negative words tweeted by each candidate across all their tweets.

```{r}
small.tweets %>%
  group_by(screen_name) %>%
  summarise(pos = sum(pos.words)/sum(total.words),
            neg = sum(neg.words)/sum(total.words)) %>%
  arrange(desc(pos))
```

##### Using `quanteda` 

Now we use `quanteda` to do sentiment analysis on the whole corpus. You will see this is quite a bit easier (and much faster) than what we did above, since the developers of `quanteda` have optimised the code to perform these tasks much more efficiently.

As earlier in the course, we will convert our text to a corpus object. Note that here `corpus` takes detects some metadata, which we will use later.

```{r}
twcorpus <- corpus(tweets)
```

Now we're ready to run the sentiment analysis! First we will construct a dictionary object.

```{r}
sent_dictionary <- dictionary(list(positive = pos.words,
                                   negative = neg.words))
```

And now we apply it to the corpus in order to count the number of words that appear in each category

```{r}
toks <- tokens(twcorpus)
dfm <- dfm(toks)
sent <- dfm_lookup(dfm, sent_dictionary)
```

We can then extract the score and add it to the data frame as a new variable

```{r}
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])
```

And now start answering some descriptive questions...

```{r}
# what is the average sentiment score?
mean(tweets$score)
# what is the most positive and most negative tweet?
tweets[which.max(tweets$score),]
tweets[which.min(tweets$score),]
# what is the proportion of positive, neutral, and negative tweets?
tweets$sentiment <- "neutral"
tweets$sentiment[tweets$score<0] <- "negative"
tweets$sentiment[tweets$score>0] <- "positive"
table(tweets$sentiment)
```

We can also compute it at the candidate level by taking the average of the sentiment scores:

```{r}
# loop over candidates
candidates <- c("realDonaldTrump", "HillaryClinton", "tedcruz", "BernieSanders")

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}
```

But what happens if we now run the analysis excluding a single word?

```{r}
pos.words <- pos.words[-which(pos.words=="great")]

sent_dictionary <- dictionary(list(positive = pos.words,
                                   negative = neg.words))
toks <- tokens(twcorpus)
sent <- dfm_lookup(dfm(toks), sent_dictionary)
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}

```

How would we normalize by text length? (Maybe not necessary here given that tweets have roughly the same length.)

```{r}
# collapse by account into 4 documents
toks <- tokens(twcorpus)
twdfm <- dfm(toks)
twdfm <- dfm_group(twdfm, groups = screen_name)
twdfm

# turn word counts into proportions
twdfm[1:4, 1:10]
twdfm <- dfm_weight(twdfm, scheme="prop")
twdfm[1:4, 1:10]

# Apply dictionary using `dfm_lookup()` function:
sent <- dfm_lookup(twdfm, dictionary = sent_dictionary)
sent
(sent[,1]-sent[,2])

```

Finally, let's apply a different dictionary so that we can practice with dictionaries in different formats:

```{r}
data(data_dictionary_MFD)
# dictionary keys
names(data_dictionary_MFD)
# looking at words within first key
data_dictionary_MFD$care.virtue[1:10]

# applying dictionary
# 1) collapse by account
toks <- tokens(twcorpus)
twdfm <- dfm(toks)
twdfm <- dfm_group(twdfm, groups = screen_name)
# 2) turn words into proportions
twdfm <- dfm_weight(twdfm, scheme="prop")
# 3) apply dictionary
moral <- dfm_lookup(twdfm, dictionary = data_dictionary_MFD)

# are liberals more sensitive to care and virtue?
dfm_sort(moral[,'care.virtue']*100, margin='documents')
dfm_sort(moral[,'fairness.virtue']*100, margin='documents')

# are conservatives more sensitive to sanctity and authority?
dfm_sort(moral[,'sanctity.virtue']*100, margin='documents')
dfm_sort(moral[,'authority.virtue']*100, margin='documents')
```


### Exercise 2 Keyness analysis

#### Identifying discriminating words from lecture examples

The built-in corpus of inaugural addresses can also be used when we want to find words that best helped us distinguish Trump's 2017 inaugural address from all other post war presidential inaugural addresses. Why might we want to do this? For example, it gives us a sense for what kind of language is distinctively "Trumpian." We could also look to see which words most discriminate among Democrats and Republicans.

Now let's look at the code used to do this. In this section, we will be calculating discriminating words "manually" (meaning without using `quanteda`s simple function). This is to help you understand conceptually what is happening in the calculations. 

First, let's load the corpus and create a tokens object after we do a bit of preprocessing.

```{r}
library("quanteda")
library("tidyverse")
# Load the corpus
inaug.corp <- data_corpus_inaugural
# Add a new variable to the corpus (called a "docvar") that indicates whether a speech was a Trump speech or not
inaug.corp$Trump <- ifelse(inaug.corp$President=="Trump","Trump","Not.Trump")
# Remove all speeches before 1945 because we're looking only at post-war presidential inaugurations
inaug.corp <- corpus_subset(inaug.corp, Year > 1945)
# Now create a tokens object doing some simple pre-processing
inaug.toks <- inaug.corp %>%
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>%
  tokens_wordstem()
```

How many features are in this DFM? How many documents? Notice here that to use the `nfeat` function to count features, we need to make the tokens object into a dfm.

```{r}
# Print the number of documents and features in our corpus after preprocessing
print(ndoc(dfm(inaug.toks))) # number of documents
print(nfeat(dfm(inaug.toks))) # number of features
```

To calculate which words are most discriminating, you need to go word-by-word and calculate a score. Then you can rank or arrange words by this score. Let's start with one word, `america` and calculate its score. Before doing that, we need to create the contingency table, where the row indicates the category (Trump versus Not Trump) and the column counts occurances of either `america` or any word except `america`. We can create this contingency table by "collapsing" the DFM. First, use the `tokens_lookup()` function to group all words that are not `america`. Then use the `dfm_groups()` function to collapse all non-Trump rows together by summing across those rows.

```{r}
cont.tab <- inaug.toks %>% 
  tokens_lookup(dictionary(list(america = c("america"))), 
                nomatch = "not.america") %>% 
  dfm() %>% 
  dfm_group(groups = Trump)
```

It's much easier to work with contingency tables that are in a standard tabular format than in a DFM format, so let's convert it to a matrix:

```{r}
cont.tab <- cont.tab %>%
  convert("matrix")
```

There are two methods for calculating discriminating words scores with a contingency table. First are statistical association measures we introduced in class that do not rely on a language model. Second are so-called "fightin' words" scores that use a language model. Let's look at each in turn.

##### Statistical association measures (using an independence assumption)

Statistical association measures start by creating a hypothetical contingency table that has expected word counts under an **independence assumption** that the particular word being considered (here: `america`) is uncorrelated with the category.

We can calculate the four cells of the hypothetical contingency table as follows:

```{r}
# Some basic counts used to calculate probabilities
Nn <- sum(cont.tab[,2]) # number of times tokens other than `america` appear
Na <- sum(cont.tab[,1]) # number of times `america` appears
NO <- sum(cont.tab[1,]) # number of tokens spoken by Other Presidents
NT <- sum(cont.tab[2,]) # number of tokens spoken by Trump
N <- sum(cont.tab) # total tokens in corpus

# Independence model probabilities for each cell of the hypo contingency table
p11 <- (NO/N) * (Na/N)
p12 <- (NO/N) * (Nn/N)
p21 <- (NT/N) * (Na/N)
p22 <- (NT/N) * (Nn/N)

# Hypothetical contingency table
hcont.tab <- rbind(c(p11*N, p12*N),
                   c(p21*N, p22*N))

# Label things nicely
row.names(hcont.tab) <- row.names(cont.tab)
colnames(hcont.tab) <- colnames(cont.tab)

print(hcont.tab)
```

There is actually a shortcut for producing this hypothetical contingency table once you make the real contingency table. Look at the docs for the `chisq.test()` function to learn more.

```{r}
# ?chisq.test # <- To learn more
hcont.tab <- chisq.test(cont.tab)[["expected"]]
```

Using both the contingency table and the hypothetical contingency table we can calculate three statistics that will help us quantify how discriminating the word `america` is. Recall the formulas from lecture:

**Point-wise mutual information**:
$$
\text{pmi}_{kj} = \log \left(\frac{W_{kj}}{{E}_{kj}} \right)
$$

**Likelihood ratio statistic**:
$$
G^2_{kj} = 2 \times \sum_k \sum_j \left (W_{kj} \times \log \left(\frac{W_{kj}}{{E}_{kj}} \right)\right)
$$

**Pearson's $\chi^2$ statistic**:
$$
\chi^2_{kj} = \sum_k \sum_j \left(\frac{\left(W_{kj} - {E}_{kj}\right)^2}{{E}_{kj}}\right)
$$

We can do these calculations with our data as follows

```{r}
## PMI
pmi.america <- log(cont.tab[2,1]/hcont.tab[2,1]) # why do we use element 2,1 from each table? (look back at formula for PMI)

## Likelihood ratio statistic
g2.america <- 2 * sum(cont.tab * log(cont.tab/hcont.tab))

## Pearson's chi2 statistic
chi2.america <- sum((cont.tab - hcont.tab)^2/hcont.tab)

message("PMI: ", pmi.america)
message("Likelihood ratio: ", g2.america)
message("Pearson's chi2: ", chi2.america)
```

You can also do this easily in `quanteda`, which calls this measuring the "keyness" of words (rather than measuing how discriminating they are).

```{r}
library("quanteda.textstats") # need this for the keyness function

## PMI
inaug.toks %>%
  dfm() %>%
  textstat_keyness(measure = "pmi",
                   target = "2017-Trump",
                   correction = "none") %>%
  filter(feature=="america")

## Likelihood ratio statistic
inaug.toks %>%
  dfm() %>%
  textstat_keyness(measure = "lr",
                   target = "2017-Trump",
                   correction = "none") %>%
  filter(feature=="america")

## Pearson's chi2 statistic
inaug.toks %>%
  dfm() %>%
  textstat_keyness(measure = "chi2",
                   target = "2017-Trump",
                   correction = "none") %>%
  filter(feature=="america")
```

### Fightin' words

The fightin' words approach uses a model of language to achieve a similar outcome as the statistical association measures. 

First, you need to calculate the probability that the word $j$ will occur in each category $k$ using this formula:
$$
\widehat{\mu}_{kj} = \frac{W_{kj} + \alpha_j}{W_{k} + \sum_j \alpha_j}
$$
For the example we're considering here, the two "categories" are Trump and all other postwar presidents, so for each word, we will need to calculate this number for each of the two categories. Typically (and definitely in this course), you set $\alpha_j = 1$:
$$
\widehat{\mu}_{kj} = \frac{W_{kj} + 1}{W_{k} + 2}
$$
Now, let's calculate these probabilities for the word `nation` using our data from the contingency table.
```{r}
mu.nation.Oth <- (cont.tab[1,1]+1)/(sum(cont.tab[1,])+2)
mu.nation.Trump <- (cont.tab[2,1]+1)/(sum(cont.tab[2,])+2)
```

For a particular word, after you calculate the probabilities (as above), then you then use them to calculate the $z$-score for the word. Recall from lecture, this is a three step process. First, calculate this:
$$
\hat{\delta}_{j}^{(k-k')} = \log\left(\frac{\widehat{\mu}_{kj}}{1-\widehat{\mu}_{kj}}\right) - \log\left(\frac{\widehat{\mu}_{k'j}}{1-\widehat{\mu}_{k'j}}\right)
$$
Then this:
$$
\textrm{Var}\left(\hat{\delta}_{j}^{(k-k')}\right) \approx \frac{1}{W_{kj} + \alpha_{j}} + \frac{1}{W_{k'j} + \alpha_{j}} 
$$
Finally the $z$-score:
$$
\hat{z}_{j}^{(k-k')} = \frac{\hat{\delta}_{j}^{(k-k')}}{\sqrt{\textrm{Var}\left(\hat{\delta}_{j}^{(k-k')}\right)}} 
$$
Let's do this for the word `nation` in our data.
```{r}
# Step 1, calculate delta
delta.nation.DvsR <- log(mu.nation.Oth/(1-mu.nation.Oth)) - log(mu.nation.Trump/(1-mu.nation.Trump))
# Step 2, calculate Var(delta)
var.delta.nation.DvsR <- 1/(1+cont.tab[1,1]) + 1/(1+cont.tab[2,1])
# Step 3, calculate z-score
z.nation.DvsR <- delta.nation.DvsR/sqrt(var.delta.nation.DvsR)
print(z.nation.DvsR)
```

You can see here that the $z$-score (the "fightin' words" score) for the word `nation` is approximately 1.084. Since this is positive, this indicates higher use by presidents other than Trump (see how we calculated $\hat{\delta}$). 

##### Statistical significance

All of these methods for measuring discriminating words can be interpreted in terms of statistical significance. (PMI is a bit more difficult, so let's set that one aside.)

- Both the Likelihood ratio statistic and the Pearson's chi2 statistic are distributed according to a chi2 distribution with one degree of freedom. You can find the associated threshold test statistic and p-value using a standard table, like the one here: <https://math.arizona.edu/~jwatkins/chi-square-table.pdf>
- The "fightin' words" score is simply a $z$-score, and you can use a z-score table to figure out the p-values. Rule of thumb is that a z-score above 2 (more precisely, 1.96) or below -2 (more precisely -1.96) is statistically signifcant at the 0.05 level


#### Identifying most unique features of documents

We will again look at a corpus of tweets posted by the four leading candidates in the 2016 US Presidential primaries. Let's use `quanteda` to measure the "keyness" of the words in the tweets. First, let's load the data, add a variable the party of the candidate, and turn into a corpus.

```{r}
# library("quanteda.textplots")
library("quanteda.textstats")

tweets <- read_csv('candidate-tweets.csv')
tweets$Party <- ifelse(grepl("(Bernie|Hillary)", tweets$screen_name), "Democrat", "Republican")

twcorpus <- tweets %>%
  corpus()
```

Next, let's create a tokens object, doing a little "standard" preprocessing.

```{r}
twtoks <- twcorpus %>%
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>%
  tokens_wordstem()
```

Now we're ready to start measuring discriminating words. First, we need to decide what categories we want to use to measure. One thing we could do is just look at each candidate individually and ask, "how distinctive is their rhetoric relative to the others?" To do this, we need to create a DFM, collapsing it by the categories we want to discriminate among. Since `screen_name` indicates which candidate, we'll use that:

```{r}
twdfm <- twtoks %>% 
  dfm() %>%
  dfm_group(groups=c(screen_name))
```

Now, let's look at the top discriminating words for each candidate using the Pearson's chi2 measure:

```{r}
# Donald Trump
textstat_keyness(twdfm, target="realDonaldTrump", measure="chi2") %>%
  head(n=20)
# Hillary Clinton
textstat_keyness(twdfm, target="HillaryClinton", measure="chi2") %>%
  head(n=20)
# Ted Cruz
textstat_keyness(twdfm, target="tedcruz", measure="chi2") %>%
  head(n=20)
# Bernie Sanders
textstat_keyness(twdfm, target="BernieSanders", measure="chi2") %>%
  head(n=20)
```

Looking at these suggests we should do some more data cleaning!

Now let's look to see how the rhetoric of tweets differed across party. We now collapse the DFM by the party variable and not the screen_name.

```{r}
twdfm <- twtoks %>% 
  dfm() %>%
  dfm_group(groups=c(Party)) 
```

Let's calculate Pearson's chi2:

```{r}
chi2.dem <- twdfm %>% 
  textstat_keyness(target="Democrat", measure="chi2") %>%
  tibble()
```

Finally, let's plot this. We'll only plot the top 10 and bottom 10 most discriminating words.

```{r}
extremes <- c(1:10,(nrow(chi2.dem)-10):nrow(chi2.dem))
chi2.dem  %>%
  select(feature, chi2) %>%
  mutate(feature = factor(feature, levels = rev(feature))) %>%
  mutate(Document = ifelse(chi2 > 0, "Democratic Candidates", "Republican Candidates")) %>%
  filter(row_number() %in% extremes) %>%
  ggplot() +
  labs(title = "Most and least indicative of Democrats") +
  xlab("Pearson's Chi2 Statistic") + 
  scale_fill_manual(values = c("blue", "red")) + 
  geom_col(aes(y = feature,  x = chi2, group = Document, fill = Document)) +
  theme_bw() + 
  geom_vline(xintercept = -3.841, linetype = "dashed") + 
  geom_vline(xintercept = 3.841, linetype = "dashed")
```



### Exercise 3 Classifiers

#### Naive Bayes

The code here illustrates how we can use supervised machine learning to predict categories for unseen documents based on a set of labeled documents. We use quanteda and its textmodels. Please note that cross-validation is not embedded in its textmodels, you can either manually split, estimate, and validation for k folds, or use crossval function see example here: https://rdrr.io/github/quanteda/quanteda.classifiers/src/R/crossval.R.

Our running example will focus on whether we can predict gender based on the character distribution of first names. Note that for this example, each document is a specific name (one word!), and each feature is a specific character in each name. This is a bit different than what we've done so far (treating words as document features), but all the same principles apply equally.

The file `EN-names.csv` contains a list of nearly 25,000 popular names in the US labeled by the most frequent gender based on Social Security records. Let's read this dataset into R, convert it into a corpus with gender as a document-level variable.

```{r}
library("tidyverse")
library("quanteda")
d <- read_csv("EN-names.csv")

# creating corpus object
cnames <- corpus(d, text_field = "name")
docvars(cnames, "gender") <- d$gender
```

We will need to specify what the training set and test set will be. In this case, let's just take an 80% random sample of names as training set and the rest as test set, which we will use to compute the performance of our model. We will then create a document-feature matrix where each feature is a character.

```{r}
# shuffling to split into training and test set
smp <- sample(c("train", "test"), size=ndoc(cnames), prob=c(0.80, 0.20), replace=TRUE)
train <- which(smp=="train")
test <- which(smp=="test")

# tokenizing and creating DFM
characters <- tokens(cnames, what="character")
namesdfm <- dfm(characters)
```

We're now ready to train our model! Let's start with a Naive Bayes model using the `textmodel_nb()` function:

```{r}
#install.packages('quanteda.textmodels')
library("quanteda.textmodels")

# training Naive Bayes model
nb <- textmodel_nb(namesdfm[train,], docvars(cnames, "gender")[train])
# predicting labels for test set
preds <- predict(nb, newdata = namesdfm[test,])
# computing the confusion matrix
cm <- table(docvars(cnames, "gender")[test], preds) # note: this will put true class in rows, predicted class in columns
```

How well did we do? We can compute precision, recall, and accuracy to quantify it.

```{r}
# function to compute performance metrics
precrecall <- function(mytable, verbose=TRUE) {
    truePositives <- mytable[1,1]
    falsePositives <- sum(mytable[,1]) - truePositives
    falseNegatives <- sum(mytable[1,]) - truePositives
    precision <- truePositives / (truePositives + falsePositives)
    recall <- truePositives / (truePositives + falseNegatives)
    if (verbose) {
        print(mytable)
        cat("\n precision =", round(precision, 2), 
            "\n    recall =", round(recall, 2), "\n")
    }
    invisible(c(precision, recall))
}

# precision and recall
precrecall(cm)
# accuracy
sum(diag(cm)) / sum(cm)
```

Hmm, not terribly great. But what if we try with character n-grams up to bigrams instead of unigrams?

```{r}
characters <- tokens_ngrams(characters, n=1:3)


namesdfm <- dfm(characters)
namesdfm <- dfm_trim(namesdfm, min_docfreq = 20)
namesdfm
# Naive Bayes model
nb <- textmodel_nb(namesdfm[train,], docvars(cnames, "gender")[train])
preds <- predict(nb, newdata = namesdfm[test,])
(cm <- table(preds, docvars(cnames, "gender")[test]))
# performance
precrecall(cm) # precision, recall
sum(diag(cm)) / sum(cm) # accuracy

```

Slightly better! We can dig a bit more into the model by extracting the posterior class probabilities for specific characters.

```{r}
# extracting posterior word probabilities
get_posterior <- function(nb) {
  PwGc <- nb$param
  Pc <- nb$priors
  PcGw <- PwGc * base::outer(Pc, rep(1, ncol(PwGc)))
  PcGw <- matrix(sapply(PcGw, function(x) sqrt(sum(x^2))), nrow=2, dimnames = dimnames(PwGc))
  names(dimnames(PcGw))[1] <- names(dimnames(PwGc))[1] <- "classes"
  PwGc
}
probs <- get_posterior(nb)
probs[,c("a", "o", "e")]
```

#### Regularized regression

We'll now switch to the other type of classifier we saw in the lecture - a regularized regression. This model is not implemented in quanteda, but we can use one of the other available packages in R. For regularized regression, glmnet is one of the best option, since it tends to be faster than caret or mlr, and it has cross-validation already built-in, so we don’t need to code it from scratch. 

We'll start with a ridge regression:

```{r}
# install.packages("glmnet")
library(glmnet)

ridge <- cv.glmnet(x=namesdfm[train,], y=docvars(cnames, "gender")[train],
                   alpha=0, nfolds=5, family="binomial")
```

We use the `cv.glmnet()` function, with the following options: `alpha` indicates whether we want a ridge penalty (`alpha=0`) or a lasso penalty (`alpha=1`), `nfolds` is the number of K folds for the cross-validation procedure, and `family` indicates the type of classifier (`binomial` means binary here).

It's generally good practice to plot the results of the cross-validation procedure.

```{r}
plot(ridge)
```

What do we learn from this plot? It shows the error (with confidence intervals based on the cross-validation procedure) for each possible value of lambda (the penalty parameter). The numbers on top indicate the number of features (which remain constant with ridge, unlike with lasso). We generally find that increasing the penalty parameter actually hurts.

Let's now compute different performance metrics to see how we're doing now.

```{r}
pred <- predict(ridge, namesdfm[test,], type="class")
(cm <- table(pred, docvars(cnames, "gender")[test]))

# performance metrics
precrecall(cm) # precision, recall
sum(diag(cm)) / sum(cm) # accuracy
```

Not bad! And with a regularized regression, in a similar way as we did earlier with the Naive Bayes model, we can also extract the feature-specific coefficients to try to understand how the latent dimension we're capturing here can be interpret.

```{r}
# extracting coefficients
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]

## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                ngram = names(beta), stringsAsFactors=F)

# lowest and highest coefficients
df <- df[order(df$coef),]
head(df[,c("coef", "ngram")], n=10)
tail(df[,c("coef", "ngram")], n=10)
```

The code below shows how to re-run the analysis but this time with lasso. Note that this time the number of features will change depending on the value of the penalty parameter.

```{r}
# now with lasso
lasso <- cv.glmnet(x=namesdfm[train,], y=docvars(cnames, "gender")[train],
                   alpha=1, nfolds=5, family="binomial")
plot(lasso)

pred <- predict(lasso, namesdfm[test,], type="class")
(cm <- table(pred, docvars(cnames, "gender")[test]))

# precision and recall
precrecall(cm)
# accuracy
sum(diag(cm)) / sum(cm)

# extracting coefficients
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]

## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                ngram = names(beta), stringsAsFactors=F)

# note that some features become 0
table(df$coef==0)

df <- df[order(df$coef),]
head(df[,c("coef", "ngram")], n=10)
tail(df[,c("coef", "ngram")], n=10)
```
